{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d097c6",
   "metadata": {},
   "source": [
    "# DATA WRANGLING REPORT ON THE WE_RATE_DOGS TWITTER ARCHIVE\n",
    "\n",
    "\n",
    "The goal of the project is to wrangle WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. WeRateDogs is a Twitter account that rates people's dogs with a humorous comments about the dog. These ratings almost always have a denominator of 10. The numerators, are almost always greater than 10. 11/10, 12/10, 13/10, etc. The Twitter archive is great, but it only contains very basic tweet information. The additional gathering was done, then assessing and cleaning for worthy analyses and visualizations.\n",
    "\n",
    "The data wrangling process involves 3 procedures:\n",
    "\n",
    "1.\tData gathering\n",
    "2.\tData assessing\n",
    "3.\tData cleaning\n",
    "\n",
    "\n",
    "## DATA GATHERING\n",
    "\n",
    "The three datasets gathered are:\n",
    "\n",
    "#### 1. Enhanced Twitter Archive\n",
    "\n",
    "This file was downloaded manually by clicking the following link: twitter_archive_enhanced.csv provided in the Udacity classroom. It was then uploaded and read into a pandas DataFrame.The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything.\n",
    "\n",
    "#### 2. Tweet JSON txt File\n",
    "\n",
    "This was gathered by querying Twitter's API.  The Twitter API was gotten confidentially from Twitter. For the basicness of Twitter archives, the tweet JSON file contains the retweet count and favorite count which are notable columns needed for this project. Using the tweet IDs in the WeRateDogs Twitter archive, the Twitter API was queried for each tweet's JSON data using Python's Tweepy library, and each tweet's entire set of JSON data was stored in a file called tweet_json.txt file. Each tweet's JSON data was written to its line, then read line by line into a pandas DataFrame with tweet ID, date created, retweet count, and favorite count.\n",
    "\n",
    "#### 3. Image Predictions File\n",
    "\n",
    "This is a file that contains a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images). This file is present in each tweet according to a neural network. It was hosted on Udacity's servers and downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv. It was then uploaded and read into a pandas data frame.\n",
    "\n",
    "\n",
    "## ASSESSING DATA\n",
    "\n",
    "The three datasets were scrutinized thoroughly to assess and check for any dirtiness(quality) or tidiness issues by using both visual assessment method(viewed on Google Sheet and Microsoft Excel) and programmatic assessment method by using various codes such as .info(),.duplicated(),.isnull(),.value_counts() methods and several others.\n",
    "\n",
    "\n",
    "\n",
    "#### QUALITY ISSUES\n",
    "The dirtiness issues discovered are:\n",
    "\n",
    "[TWITTER ARCHIVE DATASET]\n",
    "1.\tThe erroneous data type of timestamp and tweet_id columns\n",
    "2.\tin_reply_to_status id and in_reply_to_user_id columns,Retweeted_status_id,retweeted_status_user_id, and retweted_sttus_timestamp columns have so many null entries\n",
    "3.\tSome rating_denominators are not 10\n",
    "4.\tExpanded_urls column has some null entries\n",
    "5.\tSome dogs without names were given a, an, the, not, by, all, my, None instead of null\n",
    "6.\tThe source column has HTTPS tag attached to it\n",
    "7.\tNulls represented as none in Doggo,floofer,pupper,puppo, and dog name columns\n",
    "8.\tThe text column has a https tag attached to it\n",
    "9.\tThe timestamp has extra zeros added to its values\n",
    "10.\tSome rating_numerators are equal to 0 and large numbers.\n",
    "\n",
    "\n",
    "[IMAGE PREDICTIONS]\n",
    "1.\tNames of dog breeds start with a capital and small letters and contain underscores instead of spaces in P1, P2, and P3 columns.\n",
    "2.\tThe erroneous data type of tweet_id columns\n",
    "3.\tMissing records\n",
    "4.\tSome predictions contain 3 false values which mean they are not dog picture tweets.\n",
    "\n",
    "\n",
    "[WERATE DOG_TWEET]\n",
    "1.\tThe erroneous data type of date created and tweet_id columns\n",
    "2.\tMissing records\n",
    "3.\tThe date-created column has extra zeros added to its values\n",
    "\n",
    "Generally,\n",
    "1.\tAll 3 datasets have a different numbers of records.\n",
    "2.\tThe timestamp and date_created columns have the same values, duplicated columns are not needed. The date_created column will be dropped.\n",
    "3.\tSome column names have to be renamed for clearer descriptions.\n",
    "\n",
    "\n",
    "\n",
    "#### TIDINESS ISSUE\n",
    "The tidiness issues discovered are:\n",
    "\n",
    "1.\tIn the Twitter archive enhanced dataset, there are redundant stage columns of dog[floofer, doggo,pupper,puppo], only one stage column is needed.\n",
    "2.\tIn the image predictions dataset, there are redundant prediction columns of dog breed and [p1,p2,p3], only the right prediction breed column and prediction confidence are needed.\n",
    "3.\t3 separate datasets are not needed, one table is enough.\n",
    "\n",
    "## DATA CLEANING\n",
    "\n",
    "A copy of each dataset was made before cleaning and the define-code-test framework was used with the cleaning methods documented. These issues were addressed by using various cleaning techniques or methods. Some of the methods used include the .astype() method to change the datatype, merge() method to combine datasets, for loop for iteration,  regular expressions to extract and replace some words , the str.lower() and str. title() methods to change the capitalization of the words, the .query() method to query the data frame , the groupby() method to group different columns together and so on.\n",
    "\n",
    "## STORING DATA\n",
    "\n",
    "The cleaned dataset was then stored as the twitter_archive_master.csv using the to_csv() method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459ff62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
